from llama_cpp import Llama
import json
import re
import time

# --- é…ç½® ---
model_path = "./finetune_model/qwen3_0.6B_q4_k_m.gguf" 
UNIFIED_INSTRUCTION = "æ™ºèƒ½å®¶å±…ä¸­æ§ï¼šæå–ç”¨æˆ·æŒ‡ä»¤ä¸­çš„å®ä½“ä¸æ„å›¾ï¼Œè¾“å‡ºæ ‡å‡†çš„JSONæ§åˆ¶ä»£ç ã€‚"

# --- åŠ è½½æ¨¡å‹ ---
print("æ­£åœ¨åŠ è½½ GGUF æ¨¡å‹...")
llm = Llama(
    model_path=model_path,
    n_ctx=512,        # ç¨å¾®è°ƒå°ä¸€ç‚¹ï¼Œå¤Ÿç”¨å°±è¡Œ
    n_gpu_layers=0, 
    n_threads=4,       # æ ‘è“æ´¾5 ç‰©ç†æ ¸å¿ƒæ•°
    n_batch=512,       
    use_mmap=False,    # ã€ä¿®æ”¹ç‚¹ã€‘å…³é—­ mmapï¼Œå¼ºåˆ¶åŠ è½½è¿›å†…å­˜ï¼Œé¿å… SD å¡æ…¢é€Ÿå½±å“//é¢„çƒ­æ•ˆæœ
    verbose=False      
)

def predict(user_input, is_warmup=False):
    messages = [
        {"role": "system", "content": "æ™ºèƒ½å®¶å±…ä¸­æ§ï¼šæå–ç”¨æˆ·æŒ‡ä»¤ä¸­çš„å®ä½“ä¸æ„å›¾ï¼Œè¾“å‡ºæ ‡å‡†çš„JSONæ§åˆ¶ä»£ç ã€‚"},
        {"role": "user", "content": f"ä»»åŠ¡ï¼š{UNIFIED_INSTRUCTION}\næŒ‡ä»¤ï¼š{user_input}"}
    ]
    
    start_time = time.time()
    
    output = llm.create_chat_completion(
        messages=messages,
        max_tokens=512, 
        temperature=0.1,
    )
    
    end_time = time.time()
    total_time = end_time - start_time
    
    # è·å–è¯¦ç»†è€—æ—¶
    timing = output['usage']
    prompt_tokens = timing['prompt_tokens']
    completion_tokens = timing['completion_tokens']
    
    # æ³¨æ„ï¼šllama-cpp-python çš„ output å¯¹è±¡é‡Œå…¶å®ä¸ç›´æ¥åŒ…å« eval_timeï¼Œ
    # æˆ‘ä»¬ä¸»è¦é æ€»æ—¶é—´æ¥åˆ¤æ–­ï¼Œæˆ–è€…å¼€å¯ verbose=True çœ‹åº•å±‚æ—¥å¿—ã€‚
    # è¿™é‡Œæˆ‘ä»¬ä¸»è¦çœ‹ç¬¬äºŒæ¬¡è¿è¡Œçš„æ€»æ—¶é—´ã€‚

    prefix = "[é¢„çƒ­]" if is_warmup else "[æ­£å¼]"
    print(f"{prefix} è€—æ—¶: {total_time:.4f} ç§’ | ç”Ÿæˆ: {completion_tokens} tokens")
    
    return output['choices'][0]['message']['content']

# --- 1. é¢„çƒ­ (Warm-up) ---
# è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼Œè®©å†…å­˜å’Œ CPU å‡†å¤‡å¥½
print("\nğŸ”¥ æ­£åœ¨é¢„çƒ­ (Warm-up)... ç¬¬ä¸€æ¬¡è¿è¡Œé€šå¸¸è¾ƒæ…¢")
for i in range(10):
    print(f"{i+1}/10 é¢„çƒ­ä¸­...", end='\r')
    predict("æˆ‘æƒ³è¦æ‰“å¼€å®¢å…çš„ç¯", is_warmup=True)

# --- 2. æ­£å¼æµ‹è¯• ---
print("\n=== ğŸš€ æ­£å¼æµ‹è¯• (çœŸå®é€Ÿåº¦) ===")
user_text = "æŠŠå®¢å…ç¯å…³äº†ï¼Œé¡ºä¾¿æ‰“å¼€ç©ºè°ƒ"
print(f"æŒ‡ä»¤: {user_text}")#
# print(f"æŒ‡ä»¤ï¼š{user_text}")

# è¿è¡Œç¬¬ä¸€æ¬¡æ­£å¼æµ‹è¯•
result = predict(user_text)
print(f"è¾“å‡º: {result}")

# è¿è¡Œç¬¬äºŒæ¬¡æ­£å¼æµ‹è¯• (éªŒè¯ç¨³å®šæ€§)

print("\n--- å†æ¬¡æµ‹è¯• ---")
result_2 = predict("å§å®¤å¤ªçƒ­äº†ï¼Œè°ƒåˆ°24åº¦")
print(f"æŒ‡ä»¤: å§å®¤å¤ªçƒ­äº†ï¼Œè°ƒåˆ°24åº¦")
print(f"è¾“å‡º: {result_2}")
print("\n=== æµ‹è¯•ç»“æŸ ===\n")

